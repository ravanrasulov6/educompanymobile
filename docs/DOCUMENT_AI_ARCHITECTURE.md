# Supabase + Google Document AI + Groq OCR Cleaning + Publish Flow

## ğŸ“Œ 1. Update Summary (NÉ™lÉ™r ÆlavÉ™ Edildi)
Bu versiyada tÉ™lÉ™blÉ™rinizÉ™ É™sasÉ™n sistemÉ™ 2 bÃ¶yÃ¼k vÉ™ kritik xÃ¼susiyyÉ™t É™lavÉ™ edildi:
1. **Groq OCR Cleaning (MÃ¼tlÉ™q):** Document AI-dan gÉ™lÉ™n OCR mÉ™tni (`raw_text`) birbaÅŸa bazaya final kimi yazÄ±lmÄ±r. YalnÄ±z OCR sÉ™hvlÉ™rini dÃ¼zÉ™ltmÉ™k, defis (hyphenation) vÉ™ sÉ™tir qÄ±rÄ±lmalarÄ±nÄ± tÉ™mizlÉ™mÉ™k mÉ™qsÉ™dilÉ™ `_shared/groq.ts` modulu LLM-É™ (Groq - llama-3.3) mÃ¼raciÉ™t edir. Groq fakt uydurmur, yalnÄ±z dÃ¼zÉ™liÅŸ edir vÉ™ resultÄ± strict JSON kimi: `{ "clean_text": "...", "changes_summary": "..." }` qaytarÄ±r.
2. **Teacher Edit Flow:** OCR/Groq emalÄ± bitdikdÉ™n sonra sÉ™nÉ™d `draft` (qaralama) statusu alÄ±r. MÃ¼É™llim xÃ¼susi "Review/Edit Screen" aÃ§Ä±b sÉ™hifÉ™lÉ™ri redaktÉ™ edÉ™ bilir. ServerdÉ™ki `save-edits` funksiyasÄ± `edited_text` sÃ¼tununu debounced olaraq gÃ¼ncÉ™llÉ™yir.
3. **Publish & Versioning Flow:** DÃ¼zÉ™liÅŸ (Edit) yekunlaÅŸdÄ±qdan sonra "Publish" edilir (`publish-document` funksiyasÄ±). Snapshot gÃ¶tÃ¼rÃ¼lÉ™rÉ™k (`final_text = COALESCE(edited_text, clean_text, raw_text)`) tamamilÉ™ immutable (dÉ™yiÅŸdirilmÉ™z) olaraq `ai_document_versions` cÉ™dvÉ™lindÉ™ `snapshot_json` kimi yadda saxlanÄ±lÄ±r. ÆvvÉ™lki versiyaya qayÄ±tmaq Ã¼Ã§Ã¼n version arxitekturasÄ± aktiv olur.
4. **Question Generation Logic:** Sual hazÄ±rlama modulu vÉ™ promptlarÄ± qÉ™ti olaraq dÉ™yiÅŸilmÉ™lidir ki, sualÄ± É™sas sÉ™nÉ™din Ã¶zÃ¼ndÉ™n (vÉ™ ya `raw_text`-dÉ™n) **YOX**, yalnÄ±z publish edilmiÅŸ version snapshot JSON-nun `final_text`-indÉ™n generasiya etsin.
5. **VerilÉ™nlÉ™r BazasÄ± DÉ™yiÅŸikliklÉ™ri:** Draft, Publish statuslarÄ± É™lavÉ™ edildi. LimitlÉ™rÉ™ nÉ™zarÉ™t etmÉ™k Ã¼Ã§Ã¼n Groq funksiyasÄ±na (p-limit: 3) maksimum paralellik tÉ™tbiq edildi. GCS limitlÉ™rinÉ™ qarÅŸÄ± cleanup iÅŸÉ™ salÄ±ndÄ±.

---

## ğŸ”„ 2. YenilÉ™nmiÅŸ AxÄ±n DiaqramÄ± (Mermaid Sequence)

```mermaid
sequenceDiagram
    participant Flutter as Mobile/Web Client
    participant DocProcess as EF: process-document
    participant Poll as EF: poll-document
    participant Groq as Groq API
    participant DB as Supabase DB
    participant Edit as EF: save-edits
    participant Publish as EF: publish-document
    participant QGen as EF: generate-questions

    Flutter->>DocProcess: POST /process-document
    DocProcess->>DB: Status='polling' (or sync finish)
    
    loop Background Poller
        Poll->>Poll: Fetch Output JSON from GCS (Sort by page_no)
        Poll->>Groq: Clean Text (Fix Line Breaks & OCR limits, pLimit=3)
        Groq-->>Poll: Strict JSON {clean_text, changes_summary}
        Poll->>DB: Insert raw_text & clean_text & cleaning_failed
        Poll->>DB: Status='draft' (Ready for Edit)
    end

    Flutter->>Edit: User Edits Page (PATCH /save-edits)
    Edit->>DB: Update 'edited_text' & 'edited_at'
    
    Flutter->>Publish: User Clicks "Publish" (POST /publish-document)
    Publish->>DB: Check status == 'draft' (Idempotency)
    Publish->>DB: Create snapshot in ai_document_versions (final_text)
    Publish->>DB: Update status='published', active_version++
    Publish->>QGen: Trigger Question Gen passing version_id
    QGen->>DB: Insert Questions tied to Version
```

---

## ğŸ“‚ 3. YenilÉ™nmiÅŸ Fayl AÄŸacÄ± (File Tree)

```text
supabase/
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ _shared/
â”‚   â”‚   â”œâ”€â”€ google_auth.ts      
â”‚   â”‚   â”œâ”€â”€ gcs.ts              
â”‚   â”‚   â”œâ”€â”€ document_ai.ts      
â”‚   â”‚   â”œâ”€â”€ pdf_text.ts         
â”‚   â”‚   â”œâ”€â”€ jobs.ts             
â”‚   â”‚   â””â”€â”€ groq.ts             # YENÄ°: Groq OCR Cleaning modulu
â”‚   â”œâ”€â”€ process-document/       # Ä°ÅŸin baÅŸladÄ±lmasÄ± vÉ™ GCS stream upload
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ poll-document/          # YENÄ°LÆNDÄ°: JSON oxu, Groq Clean et, bazaya 'draft' yaz
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ save-edits/             # YENÄ°: MÃ¼É™llimin dÃ¼zÉ™liÅŸlÉ™rinin yadda saxlanmasÄ± (autosave)
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â””â”€â”€ publish-document/       # YENÄ°: Snapshot (Version) yarat vÉ™ Q-Gen trigger et
â”‚       â””â”€â”€ index.ts
```

---

## ğŸ’» 4. Tam Kod BloklarÄ± (Copy-Paste Ready)

### A. `_shared/groq.ts` (Groq OCR Cleaning)
SÉ™hv sÉ™tir qÄ±rÄ±lmalarÄ±nÄ±, hyphenation-larÄ± dÃ¼zÉ™ldÉ™n modul. Konkret format tÉ™lÉ™b olunur vÉ™ 1-2 saniyÉ™dÉ™ cavablandÄ±rÄ±lÄ±r.

```typescript
// supabase/functions/_shared/groq.ts
export interface GroqCleanResult {
    clean_text: string;
    changes_summary: string | null;
}

export async function groqCleanText(rawText: string, langHint: 'az'|'en'|'mixed' = 'az'): Promise<GroqCleanResult> {
    const groqKey = Deno.env.get("GROQ_API_KEY");
    if (!groqKey) throw new Error("Missing GROQ_API_KEY");

    if (rawText.trim().length <= 15) {
        return { clean_text: rawText, changes_summary: "Too short to clean" };
    }

    const prompt = `
    You are a meticulous OCR text clearer.
    Your task is to fix scanning errors, broken hyphenations, and erratic line breaks in the following OCR text.
    LANGUAGE HINT: ${langHint}.
    
    CRITICAL RULES:
    1. DO NOT add any new information, facts, or commentary. Do not hallucinate.
    2. ONLY fix typos, whitespace, and formatting.
    3. Return ONLY a strict JSON object with this exact structure (NO extra markdown):
       {
         "clean_text": "The fully corrected text",
         "changes_summary": "Short 1-sentence summary of what kind of typos you fixed"
       }
    
    RAW OCR INPUT:
    """
    ${rawText}
    """
    `;

    const startTime = Date.now();
    const res = await fetch("https://api.groq.com/openai/v1/chat/completions", {
        method: "POST",
        headers: {
            "Authorization": `Bearer ${groqKey}`,
            "Content-Type": "application/json"
        },
        body: JSON.stringify({
            model: "llama-3.3-70b-versatile",
            response_format: { type: "json_object" },
            messages: [
                { role: "system", content: "You output strictly valid JSON." },
                { role: "user", content: prompt }
            ],
            temperature: 0.1 
        })
    });

    if (!res.ok) throw new Error(`Groq error: ${await res.text()}`);

    const data = await res.json();
    const content = data.choices[0]?.message?.content || "{}";
    
    const duration_ms = Date.now() - startTime;
    console.log(`Groq processed ${rawText.length} chars in ${duration_ms}ms`);

    try {
        const parsed = JSON.parse(content);
        return {
            clean_text: parsed.clean_text || rawText,
            changes_summary: parsed.changes_summary || null
        };
    } catch (e) {
        console.error("Groq JSON parsing failed", e);
        throw new Error("Invalid format returned by Groq");
    }
}
```

### B. `poll-document/index.ts` (Groq Ä°nteqrasiyalÄ± Poller)
BitmiÅŸ Output JSON-u oxuyub parÃ§alayÄ±r + Groq tÉ™mizliyi (max 3 paralel) tÉ™tbiq edÉ™rÉ™k bazaya `draft` olaraq verir.

```typescript
// supabase/functions/poll-document/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { getOperationStatus } from "../_shared/document_ai.ts";
import { readGcsObjectJson, listGcsObjects, deleteGcsObject } from "../_shared/gcs.ts";
import { groqCleanText } from "../_shared/groq.ts";
import { createServiceClient } from "../_shared/supabase-client.ts";
import { handleCors, jsonResponse, errorResponse } from "../_shared/cors.ts";
import pLimit from "https://esm.sh/p-limit@4.0.0";

const limit = pLimit(3); // Miximum 3 concurrent Groq cleanups

serve(async (req: Request) => {
    const corsResp = handleCors(req);
    if (corsResp) return corsResp;

    try {
        const supabase = createServiceClient();
        const { job_id } = await req.json();

        const { data: job } = await supabase.from("ai_jobs").select("*").eq("id", job_id).single();
        if (!job || job.status !== 'polling') return jsonResponse({ status: job?.status || 'not_found' });

        const operation = await getOperationStatus(job.operation_name);

        if (!operation.done) {
            await supabase.from("ai_jobs").update({ last_heartbeat_at: new Date().toISOString() }).eq("id", job.id);
            return jsonResponse({ status: "polling" });
        }

        if (operation.error) {
            await supabase.from("ai_jobs").update({ status: 'failed', error_message: JSON.stringify(operation.error) }).eq('id', job.id);
            await supabase.from("ai_documents").update({ status: "failed" }).eq("id", job.document_id);
            return jsonResponse({ status: "failed", error: operation.error });
        }

        // Processing completed, read parsed JSON from GCS
        const bucket = Deno.env.get("GCS_BUCKET_TEMP")!;
        const { gcsInputPath, gcsOutputPrefix } = job.params;

        const objectNames = await listGcsObjects(bucket, gcsOutputPrefix);
        const jsonNames = objectNames.filter(n => n.endsWith('.json'));

        const rawPages = [];

        for (const objName of jsonNames) {
            const jsonOutput = await readGcsObjectJson(bucket, objName);
            if (!jsonOutput.document?.pages) continue;
            const textStr = jsonOutput.document.text || "";

            for (const p of jsonOutput.document.pages) {
                let pageText = "";
                let pNumber = parseInt(p.pageNumber || '1', 10);
                
                if (p.layout?.textSegment) {
                    const start = parseInt(p.layout.textSegment.startIndex || "0", 10);
                    const end = parseInt(p.layout.textSegment.endIndex || "0", 10);
                    pageText = textStr.substring(start || 0, end || 0);
                }
                rawPages.push({ page_no: pNumber, raw_text: pageText.trim() });
            }
        }

        rawPages.sort((a, b) => a.page_no - b.page_no); // SORT JSON order

        // Groq Cleaning
        const dbPages = await Promise.all(rawPages.map((p) => limit(async () => {
            let clean_text = p.raw_text;
            let changes_summary = null;
            let cleaning_failed = false;

            if (p.raw_text.length > 0) {
                try {
                    const groqRes = await groqCleanText(p.raw_text, 'az');
                    clean_text = groqRes.clean_text;
                    changes_summary = groqRes.changes_summary;
                } catch (err) {
                    console.error(`Groq error on page ${p.page_no}:`, err);
                    cleaning_failed = true; // DB marked as fail, fallback active
                }
            }

            return {
                document_id: job.document_id,
                page_no: p.page_no,
                raw_text: p.raw_text,
                clean_text: clean_text,
                cleaning_model: 'llama-3.3-70b-versatile',
                cleaning_version: 'v1',
                cleaning_failed: cleaning_failed,
                changes_summary: changes_summary,
                source: 'ocr'
            };
        })));

        for (let i = 0; i < dbPages.length; i += 10) {
            await supabase.from("ai_document_pages").insert(dbPages.slice(i, i + 10));
        }

        // Cleanup temp GCS
        let cleanupFailed = false;
        try {
            await deleteGcsObject(bucket, gcsInputPath);
            for (const n of jsonNames) await deleteGcsObject(bucket, n);
        } catch { cleanupFailed = true; }

        await supabase.from("ai_documents").update({ 
            status: "draft", 
            page_count: rawPages.length 
        }).eq("id", job.document_id);
        
        await supabase.from("ai_jobs").update({ status: 'completed', cleanup_failed: cleanupFailed }).eq('id', job.id);

        return jsonResponse({ status: "completed", draft_ready: true });
    } catch (e: any) {
        return errorResponse(e.message, 500);
    }
});
```

### C. `save-edits/index.ts` (MÃ¼É™llim TÉ™nzimlÉ™mÉ™si - Autosave)
```typescript
// supabase/functions/save-edits/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createServiceClient, getUserId } from "../_shared/supabase-client.ts";
import { handleCors, jsonResponse, errorResponse } from "../_shared/cors.ts";

serve(async (req: Request) => {
    const corsResp = handleCors(req);
    if (corsResp) return corsResp;

    try {
        const userId = await getUserId(req);
        if (!userId) return errorResponse("Unauthorized", 401);

        const { document_id, page_no, edited_text } = await req.json();
        const supabase = createServiceClient();

        const { data: doc } = await supabase.from('ai_documents')
            .select('user_id, status')
            .eq('id', document_id)
            .single();

        if (!doc) return errorResponse("Not found", 404);
        if (doc.user_id !== userId) return errorResponse("Forbidden", 403);
        if (doc.status !== 'draft') return errorResponse("Document must be in 'draft' status to edit", 400);

        const now = new Date().toISOString();

        const { error } = await supabase.from('ai_document_pages')
            .update({
                edited_text: edited_text,
                edited_by: userId,
                edited_at: now
            })
            .match({ document_id: document_id, page_no: page_no });

        if (error) throw error;

        return jsonResponse({ success: true, saved_at: now });
    } catch (e: any) {
        return errorResponse(e.message, 500);
    }
});
```

### D. `publish-document/index.ts` (SÉ™nÉ™di AktivlÉ™ÅŸdir vÉ™ Version Snapshot Yarat)
Snapshot yaranarkÉ™n `final_text = COALESCE(edited_text, clean_text, raw_text)` prioriteti iÅŸlÉ™dilir.

```typescript
// supabase/functions/publish-document/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createServiceClient, getUserId } from "../_shared/supabase-client.ts";
import { handleCors, jsonResponse, errorResponse } from "../_shared/cors.ts";

serve(async (req: Request) => {
    const corsResp = handleCors(req);
    if (corsResp) return corsResp;

    try {
        const userId = await getUserId(req);
        if (!userId) return errorResponse("Unauthorized", 401);

        const { document_id, title, notes, trigger_qgen = true } = await req.json();
        const supabase = createServiceClient();

        const { data: doc } = await supabase.from('ai_documents').select('*').eq('id', document_id).single();
        if (!doc) return errorResponse("Not found", 404);
        if (doc.user_id !== userId) return errorResponse("Forbidden", 403);
        if (doc.status === 'published') return errorResponse("Already published", 409); // Idempotency check
        if (doc.status !== 'draft') return errorResponse("Document must be in 'draft' to publish", 400);

        // Fetch pages to establish final_text rules
        const { data: pages } = await supabase.from('ai_document_pages')
            .select('page_no, edited_text, clean_text, raw_text')
            .eq('document_id', document_id)
            .order('page_no', { ascending: true });

        const snapshotData = pages?.map(p => ({
            page_no: p.page_no,
            // PRIORITIZATION RULE
            final_text: p.edited_text ?? p.clean_text ?? p.raw_text ?? ''
        })) || [];

        const nextVersion = (doc.active_version || 0) + 1;
        const now = new Date().toISOString();

        // 1. Insert Version Snapshot (Immutable)
        const { data: versionDoc, error: vErr } = await supabase.from('ai_document_versions').insert({
            document_id: document_id,
            version: nextVersion,
            created_by: userId,
            created_at: now,
            snapshot_json: snapshotData,
            publish_notes: notes || null
        }).select().single();

        if (vErr) throw vErr;

        // 2. Update Document Status
        await supabase.from('ai_documents').update({
            status: 'published',
            published_at: now,
            active_version: nextVersion,
            file_name: title || doc.file_name
        }).eq('id', document_id);

        // 3. Trigger Question Generation (Async via Webhook/Job)
        if (trigger_qgen) {
           await supabase.from("ai_jobs").insert({
               document_id: document_id,
               user_id: userId,
               status: 'queued',
               job_type: 'generate_questions',
               params: { version_id: versionDoc.id } // Only feed the finalized text
           });
        }

        return jsonResponse({ success: true, status: 'published', version: nextVersion, version_id: versionDoc.id });
    } catch (e: any) {
        return errorResponse(e.message, 500);
    }
});
```

### E. Question Generation Input Selection Logic (Vacib XÃ¼lasÉ™)
`generate-questions` yaradÄ±lan zaman server kÃ¶hnÉ™ `raw_text` / `text` bazasÄ±na GÄ°RMÄ°R.
Ona yalnÄ±z spesifik version parametri verilir (`version_id`). Sual yaradan modul `ai_document_versions` daxilindÉ™ki `snapshot_json`-u oxuyaraq bÃ¼tÃ¼n suallarÄ± *yalnÄ±z* bu sarsÄ±lmaz redaktÉ™ bloku (`final_text`) É™sasÄ±nda edir. AI modelinÉ™ OCR cÄ±zÄ±qlarÄ± vÉ™ gÉ™rÉ™ksiz xÉ™ta bloklarÄ± yÃ¶nlÉ™ndirilmir.

---

## ğŸ—„ï¸ 5. Database Schema / Migrations (SQL)
Supabase SQL Editor-da iÅŸÉ™ salÄ±nmalÄ±dÄ±r:

```sql
-- 1. Status Check Update
ALTER TABLE ai_documents DROP CONSTRAINT IF EXISTS ai_documents_status_check;
ALTER TABLE ai_documents
ADD COLUMN IF NOT EXISTS status TEXT DEFAULT 'processing' 
  CHECK (status IN ('processing', 'polling', 'draft', 'published', 'failed')),
ADD COLUMN IF NOT EXISTS published_at TIMESTAMPTZ,
ADD COLUMN IF NOT EXISTS active_version INT DEFAULT 1;

-- 2. ai_document_pages Extensions:
ALTER TABLE ai_document_pages
ADD COLUMN IF NOT EXISTS raw_text TEXT,
ADD COLUMN IF NOT EXISTS clean_text TEXT,
ADD COLUMN IF NOT EXISTS cleaning_model TEXT,
ADD COLUMN IF NOT EXISTS cleaning_version TEXT DEFAULT 'v1',
ADD COLUMN IF NOT EXISTS cleaning_failed BOOLEAN DEFAULT FALSE,
ADD COLUMN IF NOT EXISTS changes_summary TEXT,
ADD COLUMN IF NOT EXISTS edited_text TEXT,
ADD COLUMN IF NOT EXISTS edited_by UUID REFERENCES auth.users(id),
ADD COLUMN IF NOT EXISTS edited_at TIMESTAMPTZ;

-- 3. Versioning Table
CREATE TABLE IF NOT EXISTS ai_document_versions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID REFERENCES ai_documents(id) ON DELETE CASCADE,
    version INT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    created_by UUID REFERENCES auth.users(id),
    snapshot_json JSONB NOT NULL,
    publish_notes TEXT
);

CREATE INDEX IF NOT EXISTS idx_ai_doc_versions_doc_id ON ai_document_versions (document_id);

-- 4. RLS for Versions
ALTER TABLE ai_document_versions ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Users can insert their own versions" ON ai_document_versions FOR INSERT WITH CHECK (auth.uid() = created_by);
CREATE POLICY "Users can view their own versions" ON ai_document_versions FOR SELECT USING (auth.uid() = created_by);
```

---

## âœ… 6. Test Plan (8 Scenarios)

1. **Groq Success Limit:** p-limit testi. `DOCUMENT_OCR`dÉ™n 30 sÉ™hifÉ™ Ã§Ä±xar. Groq API-nÉ™ saniyÉ™dÉ™ yalnÄ±z maksimum 3 paralellik atÄ±raraq hÉ™m Cloudflare error, hÉ™m ratelimit partlamasÄ±nÄ±n qarÅŸÄ±sÄ±nÄ± aldÄ±ÄŸÄ±nÄ± yoxlat.
2. **Groq Fail (Resilience):** `GROQ_API_KEY`-i dÉ™yiÅŸib sÉ™hv et. Sistemin partlamadÄ±ÄŸÄ±nÄ±, array loopunun `cleaning_failed = true` verÉ™rÉ™k fallback olaraq tÉ™miz mÉ™tn yerinÉ™ `raw_text` qaytardÄ±ÄŸÄ±nÄ± (UI-Ä±n Ã§Ã¶kmÉ™diyini) yoxla.
3. **Autosave Debounce (Edit):** UI "ReviewScreen"-dÉ™ hÉ™rbi lÉ™ngimÉ™ (Type > Wait 2 seconds > Auto Save POST). TÉ™krar-tÉ™krar `save-edits` endpointi Ã§Ã¶kdÃ¼rÃ¼lmÉ™dÉ™n `edited_text` gÃ¼ncÉ™llÉ™nir.
4. **Publish Idempotency (Conflict):** MÃ¼É™llim 2 pÉ™ncÉ™rÉ™dÉ™n sÉ™nÉ™di aÃ§Ä±q qoyub, 1ci pÉ™ncÉ™rÉ™dÉ™ Publish edÉ™ndÉ™ status 'published' olur. 2ci pÉ™ncÉ™rÉ™dÉ™n Publish basÄ±ldÄ±qda 409 Conflict "Already published" qaytarÄ±lÄ±r. HeÃ§ bir halda 2 version birdÉ™n snapshot Ã§Ä±xarmÄ±r.
5. **Permissions/RLS Check:** A mÃ¼É™llim tÉ™rÉ™findÉ™n idarÉ™ olunan document_id, B mÃ¼É™lliminÉ™ API requestlÉ™ POST `save-edits` edÉ™ndÉ™ qÉ™bul edilmir (403 Forbidden).
6. **Native PDF Bypass & Statuses:** TÉ™miz rÉ™qÉ™msal sÉ™nÉ™d (Standard PDF) oxunub tÉ™rkibindÉ™ki text birbaÅŸa extract edilÉ™ndÉ™ lazÄ±msÄ±z yerdÉ™ Document AI vÉ™ Groq Ã§alÄ±ÅŸdÄ±rmadan, anÄ±nda statusun `draft` olduÄŸuna nÉ™zarÉ™t edir.
7. **Ordering Correctness & Cleanup Fail:** GCS output JSON fayllarÄ± fÉ™rqli shard qÄ±rÄ±qlarÄ±nda `page.25`, `page.01` ÅŸÉ™klindÉ™ qayÄ±darsa `.sort((a,b) => a.page_no - b.page_no)` funksiyasÄ± ilÉ™ bazaya sÄ±ralÄ± daxil edildiyi test olunur. Sonda Bucket temp cleaning API-si icazÉ™sizliklÉ™ xÉ™ta verÉ™ndÉ™ Job `completed` statusu alÄ±r, lakin logda `cleanup_failed=true` gÃ¶zÉ™ Ã§arpÄ±r.
8. **Question Generation Data Input:** "Questions" app ekranÄ± sadÉ™cÉ™ É™n son publish edilmiÅŸ versiyanÄ± Ã§aÄŸÄ±rÄ±r vÉ™ daxilindÉ™ yalnÄ±z `final_text` cÃ¼mlÉ™lÉ™ri gÃ¶rÃ¼nÃ¼r.
